# mlds_f1_project

Here is a **clean, simple, beginner-friendly README.md** for your entire Docker + Airflow + F1 ETL pipeline.
It tells the user:

* What this project does
* How to install
* How to run
* How to update
* How the DAG works
* What each script does

Itâ€™s short, clear, and usable by anyone.

---

# âœ… **README.md (Simple & Clean)**

You can copy-paste this into your repo.

---

# **F1 ETL Pipeline â€” Airflow + Docker**

This project builds a **fully automated ETL pipeline** for Formula 1 race data using:

* Docker
* Apache Airflow
* SQLite
* OpenF1 API
* Python ETL scripts
* Tyre-change feature engineering
* (Optional) Streamlit dashboard

The Airflow DAG automatically decides whether to run a **full load** or an **incremental update**, computes tyre strategies, and runs your processing script.

---

## ğŸš€ **1. Project Overview**

This pipeline automatically:

### **ETL**

* Loads all F1 data (meetings, sessions, laps, stints, pit stops, weather, etc.)
* Maps team and driver identity history
* Builds a clean relational database (`f1_data.db`)

### **Feature Engineering**

* Computes tyre strategy events
* Calculates tyre change laps
* Produces driver-level tyre sequences and performance effects

### **Orchestration**

An Airflow DAG decides:

* **If no database exists â†’ full initial ETL**
* **If the database already exists â†’ incremental update**

After ETL, Airflow:

1. Computes tyre change features
2. Runs `app.py` (you can plug analytics or dashboard prep here)

---

## ğŸ“¦ **2. Whatâ€™s Inside**

```
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ f1_pipeline_dag.py          # Main Airflow pipeline
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ load_f1_functional.py       # Full initial ETL
â”‚   â”œâ”€â”€ update_f1_data.py           # Incremental ETL
â”‚   â”œâ”€â”€ create_tyre_changes.py      # Feature engineering
â”‚   â””â”€â”€ app.py                      # Final processing
â”œâ”€â”€ data/
â”‚   â””â”€â”€ f1_data.db                  # Created after ETL
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## ğŸ³ **3. How to Run Everything**

### **Step 1 â€” Install Docker Desktop**

Mac / Windows / Linux
[https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)

---

### **Step 2 â€” Build and Start Airflow**

In the project directory:

```bash
docker-compose down --volumes --remove-orphans
docker-compose up -d --build
```

This will:

* Build your custom Airflow image
* Install Python dependencies
* Mount your scripts/data/dags
* Start Airflow webserver + scheduler

---

### **Step 3 â€” Open Airflow UI**

Visit in browser:

```
http://localhost:8080
```

Default login:

* **user:** airflow
* **password:** airflow

---

## â–¶ **4. Running the Pipeline**

In Airflow UI:

1. Find DAG **f1_etl_pipeline**
2. Toggle it **ON**
3. Click **Trigger DAG**

Airflow will automatically choose:

### **First run â†’ full ETL**

Creates `data/f1_data.db`.

### **Later runs â†’ incremental update**

Uses your `update_f1_data.py`.

---

## ğŸ”€ **5. How the DAG Works**

```
choose_etl_mode (BranchPythonOperator)
      |
      â”œâ”€â”€ run_initial_etl   (if DB missing)
      â””â”€â”€ run_update_etl    (if DB exists)
                |
                â–¼
    compute_tyre_changes
                |
                â–¼
            run_app
```

Branch re-joining works using:

```
TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS
```

---

## âš™ï¸ **6. Common Commands**

### View running Docker containers

```bash
docker ps
```

### Stop everything

```bash
docker-compose down
```

### Rebuild everything

```bash
docker-compose up -d --build
```

### Enter Airflow container

```bash
docker exec -it airflow bash
```

---

## ğŸ“ **7. Where Data Lives**

Your SQLite database is stored on the host:

```
./data/f1_data.db
```

It is mounted inside the container at:

```
/opt/airflow/data/f1_data.db
```

So even if you stop/rebuild the container, **your data stays safe**.

---

## ğŸ“Š **8. Adding a Streamlit Dashboard (Optional)**

If your `app.py` launches Streamlit:

```
streamlit run dashboard.py --server.port 8501 --server.address 0.0.0.0
```

Expose the port in `docker-compose.yaml`:

```yaml
ports:
  - "8501:8501"
```

Then open:

```
http://localhost:8501
```

---

## ğŸ‰ **9. Youâ€™re Done!**

You now have:

* A reproducible Docker environment
* A fully automated Airflow ETL pipeline
* A tyre strategy feature engineering system
* A persistent SQLite database
* A clean DAG structure

If you want, I can also generate:

* A pretty architecture diagram
* A CLI tool for running ETL manually
* Unit tests for ETL functions
* A Streamlit dashboard UI

Just tell me!
